{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "cxjf22zmi4f6fo4qmtmk",
   "authorId": "9168516518316",
   "authorName": "BOGHBOGH",
   "authorEmail": "mahdi.askari@snowflake.com",
   "sessionId": "75dfb061-b256-4b7c-8976-264bdd2fbb72",
   "lastEditTime": 1742411508451
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "source": "# Snowpark for Python\nfrom snowflake.snowpark.types import DoubleType\nimport snowflake.snowpark.functions as F",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "source": "\n-- Using Warehouse, Database, and Schema created during Setup\nUSE WAREHOUSE ML_HOL_WH;\nUSE DATABASE ML_HOL_DB;\nUSE SCHEMA ML_HOL_SCHEMA;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7e5d0b98-75b4-4910-ae62-b7f45282a18e",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# Get Snowflake Session object\nsession = get_active_session()\nsession.sql_simplifier_enabled = True\n\n# Add a query tag to the session.\nsession.query_tag = {\"origin\":\"sf_sit-is\", \n                     \"name\":\"e2e_ml_snowparkpython\", \n                     \"version\":{\"major\":1, \"minor\":0,},\n                     \"attributes\":{\"is_quickstart\":1}}\n\n# Current Environment Details\nprint('Connection Established with the following parameters:')\nprint('User      : {}'.format(session.get_current_user()))\nprint('Role      : {}'.format(session.get_current_role()))\nprint('Database  : {}'.format(session.get_current_database()))\nprint('Schema    : {}'.format(session.get_current_schema()))\nprint('Warehouse : {}'.format(session.get_current_warehouse()))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1663684-4f61-4c26-86c0-36d884fc8211",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "# Create a Snowpark DataFrame that is configured to load data from the CSV file\n# We can now infer schema from CSV files.\ndiamonds_df = session.read.options({\"field_delimiter\": \",\",\n                                    \"field_optionally_enclosed_by\": '\"',\n                                    \"infer_schema\": True,\n                                    \"parse_header\": True}).csv(\"@DIAMONDS_ASSETS\")\n\ndiamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28b434cb-2001-4d1b-bbb2-40a47de3d1d6",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# Force headers to uppercase\nfor colname in diamonds_df.columns:\n    if colname == '\"table\"':\n       new_colname = \"TABLE_PCT\"\n    else:\n        new_colname = str.upper(colname)\n    diamonds_df = diamonds_df.with_column_renamed(colname, new_colname)\n\ndiamonds_df\ndef fix_values(columnn):\n    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n\nfor col in [\"CUT\"]:\n    diamonds_df = diamonds_df.with_column(col, fix_values(col))\n\ndiamonds_df\nlist(diamonds_df.schema)\nfor colname in [\"CARAT\", \"X\", \"Y\", \"Z\", \"DEPTH\", \"TABLE_PCT\"]:\n    diamonds_df = diamonds_df.with_column(colname, diamonds_df[colname].cast(DoubleType()))\n\ndiamonds_df\ndiamonds_df.write.mode('overwrite').save_as_table('diamonds')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11d1d113-8ef2-4338-aa42-b4e2d4b14ae2",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "\n# Snowpark for Python\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.types import DecimalType\n\n# Snowflake ML\nimport snowflake.ml.modeling.preprocessing as snowml\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.metrics.correlation import correlation\n\n# Data Science Libs\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Misc\nimport json\nimport joblib\n\n# warning suppresion\nimport warnings; warnings.simplefilter('ignore')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3738a14-e751-4253-af6a-a0de44ad670f",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "session = get_active_session()\n\n# Add a query tag to the session.\nsession.query_tag = {\"origin\":\"sf_sit-is\", \n                     \"name\":\"e2e_ml_snowparkpython\", \n                     \"version\":{\"major\":1, \"minor\":0,},\n                     \"attributes\":{\"is_quickstart\":1}}\nsession\n# First, we read in the data from a Snowflake table into a Snowpark DataFrame\n# **Change this only if you named your table something else in the data ingest notebook **\ndiamonds_df = session.table(\"DIAMONDS\")\n# Normalize the CARAT column\nsnowml_mms = snowml.MinMaxScaler(input_cols=[\"CARAT\"], output_cols=[\"CARAT_NORM\"])\nnormalized_diamonds_df = snowml_mms.fit(diamonds_df).transform(diamonds_df)\n\n# Reduce the number of decimals\nnew_col = normalized_diamonds_df.col(\"CARAT_NORM\").cast(DecimalType(7, 6))\nnormalized_diamonds_df = normalized_diamonds_df.with_column(\"CARAT_NORM\", new_col)\n\nnormalized_diamonds_df\n# Encode CUT and CLARITY preserve ordinal importance\ncategories = {\n    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n}\nsnowml_oe = snowml.OrdinalEncoder(input_cols=[\"CUT\", \"CLARITY\"], output_cols=[\"CUT_OE\", \"CLARITY_OE\"], categories=categories)\nord_encoded_diamonds_df = snowml_oe.fit(normalized_diamonds_df).transform(normalized_diamonds_df)\n\n# Show the encoding\nprint(snowml_oe._state_pandas)\n\nord_encoded_diamonds_df\n\n# Encode categoricals to numeric columns\nsnowml_ohe = snowml.OneHotEncoder(input_cols=[\"CUT\", \"COLOR\", \"CLARITY\"], output_cols=[\"CUT_OHE\", \"COLOR_OHE\", \"CLARITY_OHE\"])\ntransformed_diamonds_df = snowml_ohe.fit(ord_encoded_diamonds_df).transform(ord_encoded_diamonds_df)\n\nnp.array(transformed_diamonds_df.columns)\n# Categorize all the features for processing\nCATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\nCATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\nNUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n\ncategories = {\n    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n    \"COLOR\": np.array(['D', 'E', 'F', 'G', 'H', 'I', 'J']),\n}\n# Build the pipeline\npreprocessing_pipeline = Pipeline(\n    steps=[\n            (\n                \"OE\",\n                snowml.OrdinalEncoder(\n                    input_cols=CATEGORICAL_COLUMNS,\n                    output_cols=CATEGORICAL_COLUMNS_OE,\n                    categories=categories,\n                )\n            ),\n            (\n                \"MMS\",\n                snowml.MinMaxScaler(\n                    clip=True,\n                    input_cols=NUMERICAL_COLUMNS,\n                    output_cols=NUMERICAL_COLUMNS,\n                )\n            )\n    ]\n)\n\nPIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib'\njoblib.dump(preprocessing_pipeline, PIPELINE_FILE) # We are just pickling it locally first\n\ntransformed_diamonds_df = preprocessing_pipeline.fit(diamonds_df).transform(diamonds_df)\ntransformed_diamonds_df\n# You can also save the pickled object into the stage we created earlier for deployment\nsession.file.put(PIPELINE_FILE, \"@ML_HOL_ASSETS\", overwrite=True)\n# Set up a plot to look at CARAT and PRICE\ncounts = transformed_diamonds_df.to_pandas().groupby(['PRICE', 'CARAT', 'CLARITY_OE']).size().reset_index(name='Count')\n\nfig, ax = plt.subplots(figsize=(20, 20))\nplt.title('Price vs Carat', fontsize=28)\nax = sns.scatterplot(data=counts, x='CARAT', y='PRICE', size='Count', hue='CLARITY_OE', markers='o')\nax.grid(axis='y')\n\n# The relationship is not linear - it appears exponential which makes sense given the rarity of the large diamonds\nsns.move_legend(ax, \"upper left\")\nsns.despine(left=True, bottom=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "55272e02-9b43-4b79-9dac-06518e1d59c8",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# Snowpark for Python\nfrom snowflake.snowpark.version import VERSION\nfrom snowflake.snowpark.functions import udf\nimport snowflake.snowpark.functions as F\n\n# Snowflake ML\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml._internal.utils import identifier\n\n# data science libs\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n\n# misc\nimport json\nimport joblib\nimport cachetools\n\n# warning suppresion\nimport warnings; warnings.simplefilter('ignore')\n# Establish Secure Connection to Snowflake\nsession = get_active_session()\n\n# Add a query tag to the session.\nsession.query_tag = {\"origin\":\"sf_sit-is\", \n                     \"name\":\"e2e_ml_snowparkpython\", \n                     \"version\":{\"major\":1, \"minor\":0,},\n                     \"attributes\":{\"is_quickstart\":1}}\nsession\n# Load in the data\ndiamonds_df = session.table(\"DIAMONDS\")\ndiamonds_df\n# Categorize all the features for modeling\nCATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\nCATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\nNUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n\nLABEL_COLUMNS = ['PRICE']\nOUTPUT_COLUMNS = ['PREDICTED_PRICE']\n# Load the preprocessing pipeline object from stage- to do this, we download the preprocessing_pipeline.joblib.gz file to the warehouse\n# where our notebook is running, and then load it using joblib.\nsession.file.get('@ML_HOL_ASSETS/preprocessing_pipeline.joblib.gz', '/tmp')\nPIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib.gz'\npreprocessing_pipeline = joblib.load(PIPELINE_FILE)\n# Split the data into train and test sets\ndiamonds_train_df, diamonds_test_df = diamonds_df.random_split(weights=[0.9, 0.1], seed=0)\n\n# Run the train and test sets through the Pipeline object we defined earlier\ntrain_df = preprocessing_pipeline.fit(diamonds_train_df).transform(diamonds_train_df)\ntest_df = preprocessing_pipeline.transform(diamonds_test_df)\n# Define the XGBRegressor\nregressor = XGBRegressor(\n    input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n    label_cols=LABEL_COLUMNS,\n    output_cols=OUTPUT_COLUMNS\n)\n\n# Train\nregressor.fit(train_df)\n\n# Predict\nresult = regressor.predict(test_df)\n# Just to illustrate, we can also pass in a Pandas DataFrame to Snowflake ML's model.predict()\nregressor.predict(test_df[CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS].to_pandas())\nmape = mean_absolute_percentage_error(df=result, \n                                        y_true_col_names=\"PRICE\", \n                                        y_pred_col_names=\"PREDICTED_PRICE\")\n\nresult.select(\"PRICE\", \"PREDICTED_PRICE\")\nprint(f\"Mean absolute percentage error: {mape}\")\n\n# Plot actual vs predicted \ng = sns.relplot(data=result[\"PRICE\", \"PREDICTED_PRICE\"].to_pandas().astype(\"float64\"), x=\"PRICE\", y=\"PREDICTED_PRICE\", kind=\"scatter\")\ng.ax.axline((0,0), slope=1, color=\"r\")\n\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4bab43f-b386-418a-8430-fa2b3382f414",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "ALTER WAREHOUSE ML_HOL_WH SET WAREHOUSE_SIZE=LARGE;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38e1d85b-3041-4577-b80d-9ecb39f9a0f1",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "grid_search = GridSearchCV(\n    estimator=XGBRegressor(),\n    param_grid={\n        \"n_estimators\":[100, 200, 300, 400, 500],\n        \"learning_rate\":[0.1, 0.2, 0.3, 0.4, 0.5],\n    },\n    n_jobs = -1,\n    scoring=\"neg_mean_absolute_percentage_error\",\n    input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n    label_cols=LABEL_COLUMNS,\n    output_cols=OUTPUT_COLUMNS\n)\n\n# Train\ngrid_search.fit(train_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e8cb0ce-ddfd-4e1a-a26d-1ca158f69601",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": "ALTER WAREHOUSE ML_HOL_WH SET WAREHOUSE_SIZE=XSMALL;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4d88586-15a1-4a79-9410-371b3d5c6d84",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "print(grid_search.to_sklearn().best_estimator_)\n\n# Analyze grid search results\ngs_results = grid_search.to_sklearn().cv_results_\nn_estimators_val = []\nlearning_rate_val = []\nfor param_dict in gs_results[\"params\"]:\n    n_estimators_val.append(param_dict[\"n_estimators\"])\n    learning_rate_val.append(param_dict[\"learning_rate\"])\nmape_val = gs_results[\"mean_test_score\"]*-1\n\ngs_results_df = pd.DataFrame(data={\n    \"n_estimators\":n_estimators_val,\n    \"learning_rate\":learning_rate_val,\n    \"mape\":mape_val})\n\nsns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")\n\nplt.show()\n\n# Predict\nresult = grid_search.predict(test_df)\n\n# Analyze results\nmape = mean_absolute_percentage_error(df=result, \n                                        y_true_col_names=\"PRICE\", \n                                        y_pred_col_names=\"PREDICTED_PRICE\")\n\nresult.select(\"PRICE\", \"PREDICTED_PRICE\").show()\nprint(f\"Mean absolute percentage error: {mape}\")\n\n# Plot actual vs predicted \ng = sns.relplot(data=result[\"PRICE\", \"PREDICTED_PRICE\"].to_pandas().astype(\"float64\"), x=\"PRICE\", y=\"PREDICTED_PRICE\", kind=\"scatter\")\ng.ax.axline((0,0), slope=1, color=\"r\")\n\nplt.show()\n\noptimal_model = grid_search.to_sklearn().best_estimator_\noptimal_n_estimators = grid_search.to_sklearn().best_estimator_.n_estimators\noptimal_learning_rate = grid_search.to_sklearn().best_estimator_.learning_rate\n\noptimal_mape = gs_results_df.loc[(gs_results_df['n_estimators']==optimal_n_estimators) &\n                                 (gs_results_df['learning_rate']==optimal_learning_rate), 'mape'].values[0]\n\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "786a232d-832d-4d4b-999c-a70062cac953",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "# Get sample input data to pass into the registry logging function\nX = train_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS).limit(100)\n\n\n\ndb = identifier._get_unescaped_name(session.get_current_database())\nschema = identifier._get_unescaped_name(session.get_current_schema())\n\n# Define model name\nmodel_name = \"DIAMONDS_PRICE_PREDICTION\"\n\n# Create a registry and log the model\nnative_registry = Registry(session=session, database_name=db, schema_name=schema)\n\n# Let's first log the very first model we trained\nmodel_ver = native_registry.log_model(\n    model_name=model_name,\n    version_name='V0',\n    model=regressor,\n    sample_input_data=X, # to provide the feature schema\n    options={\"enable_explainability\": True}\n)\n\n# Add evaluation metric\nmodel_ver.set_metric(metric_name=\"mean_abs_pct_err\", value=mape)\n\n# Add a description\nmodel_ver.comment = \"This is the first iteration of our Diamonds Price Prediction model. It is used for demo purposes.\"\n\n# Now, let's log the optimal model from GridSearchCV\nmodel_ver2 = native_registry.log_model(\n    model_name=model_name,\n    version_name='V1',\n    model=optimal_model,\n    sample_input_data=X, # to provide the feature schema\n    options={\"enable_explainability\": True}\n)\n\n# Add evaluation metric\nmodel_ver2.set_metric(metric_name=\"mean_abs_pct_err\", value=optimal_mape)\n\n# Add a description\nmodel_ver2.comment = \"This is the second iteration of our Diamonds Price Prediction model \\\n                        where we performed hyperparameter optimization.\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9796a4b-a4a0-4f9b-858d-14d716f587f3",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "# Let's confirm they were added\nnative_registry.get_model(model_name).show_versions()\nnative_registry.get_model(model_name).default.version_name\nmodel_ver = native_registry.get_model(model_name).version('v1')\nresult_sdf2 = model_ver.run(test_df, function_name=\"predict\")\nresult_sdf2.show()\ntest_df.write.mode('overwrite').save_as_table('DIAMONDS_TEST')\nmv_explanations = model_ver.run(train_df, function_name=\"explain\")\nmv_explanations\nimport shap\n\nmv_explanations_pd = mv_explanations.to_pandas()\n\nmv_explanations_pd_2 = mv_explanations_pd[['CUT_OE_explanation',\n                                           'COLOR_OE_explanation',\n                                           'CLARITY_OE_explanation',\n                                           'CARAT_explanation',\n                                           'DEPTH_explanation',\n                                           'TABLE_PCT_explanation',\n                                           'X_explanation',\n                                           'Y_explanation',\n                                           'Z_explanation']]\n\n# Wrapping the explanations DataFrame into a SHAP recognized object\nshap_exp = shap._explanation.Explanation(mv_explanations_pd_2.values, \n                                         feature_names = mv_explanations_pd_2.columns)\nshap.plots.bar(shap_exp)\ntrain_df.write.mode('overwrite').save_as_table('DIAMONDS_TRAIN')",
   "execution_count": null
  }
 ]
}